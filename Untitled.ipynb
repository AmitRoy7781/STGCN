{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet.gluon import nn\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "from lib.utils import *\n",
    "\n",
    "\n",
    "class cheb_conv(nn.Block):\n",
    "    '''\n",
    "    K-order chebyshev graph convolution\n",
    "    '''\n",
    "    def __init__(self, num_of_filters, K, cheb_polys, **kwargs):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_of_filters: int\n",
    "\n",
    "        num_of_features: int, num of input features\n",
    "\n",
    "        K: int, up to K-order chebyshev polynomials will be used\n",
    "\n",
    "        '''\n",
    "        super(cheb_conv, self).__init__(**kwargs)\n",
    "        self.K = K\n",
    "        self.num_of_filters = num_of_filters\n",
    "        self.cheb_polys = cheb_polys\n",
    "\n",
    "        # shape of theta is (self.K, num_of_features, num_of_filters)\n",
    "        with self.name_scope():\n",
    "            self.Theta = self.params.get('Theta', allow_deferred_init=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Chebyshev graph convolution operation\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: mx.ndarray, graph signal matrix, shape is\n",
    "            (batch_size, num_of_features, num_of_vertices, num_of_timesteps)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mx.ndarray, shape is\n",
    "            (batch_size, self.num_of_filters,\n",
    "             num_of_vertices, num_of_timesteps)\n",
    "\n",
    "        '''\n",
    "        (batch_size, num_of_features,\n",
    "         num_of_vertices, num_of_timesteps) = x.shape\n",
    "\n",
    "        self.Theta.shape = (self.K, self.num_of_filters, num_of_features)\n",
    "        self.Theta._finish_deferred_init()\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        # ChebNet GCN will run for each time step\n",
    "        for time_step in range(num_of_timesteps):\n",
    "\n",
    "            # shape is (batch_size, num_of_features, num_of_vertices)\n",
    "            graph_signal = x[:, :, :, time_step]\n",
    "\n",
    "            output = nd.zeros(shape=(\n",
    "                self.num_of_filters, num_of_vertices, batch_size),\n",
    "                ctx=x.context)\n",
    "\n",
    "            for k in range(self.K):\n",
    "\n",
    "                # shape of T_k is (num_of_vertices, num_of_vertices)\n",
    "                T_k = self.cheb_polys[k]\n",
    "\n",
    "                # shape of theta_k is (num_of_filters, num_of_features)\n",
    "                theta_k = self.Theta.data()[k]\n",
    "\n",
    "                # shape of rhs is\n",
    "                # (num_of_features, num_of_vertices, batch_size)\n",
    "                rhs = nd.concat(*[nd.dot(graph_signal[idx], T_k)\n",
    "                                  .expand_dims(-1)\n",
    "                                  for idx in range(batch_size)], dim=-1)\n",
    "\n",
    "                output = output + nd.dot(theta_k, rhs)\n",
    "\n",
    "            # add ChebNet output to list outputs\n",
    "            outputs.append(output.transpose((2, 0, 1)).expand_dims(-1))\n",
    "\n",
    "        # concatenate all GCN output and activate them\n",
    "        return nd.relu(nd.concat(*outputs, dim=-1))\n",
    "\n",
    "\n",
    "class temporal_conv_layer(nn.Block):\n",
    "    '''\n",
    "    temporal convolution with GLU\n",
    "    '''\n",
    "    def __init__(self, num_of_filters, K_t, **kwargs):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_of_filters: int, number of temporal convolutional filters\n",
    "\n",
    "        K_t: int, length of filters\n",
    "\n",
    "        '''\n",
    "        super(temporal_conv_layer, self).__init__(**kwargs)\n",
    "\n",
    "        if isinstance(num_of_filters, int) and num_of_filters % 2 != 0:\n",
    "            raise ValueError(\n",
    "                \"num of filters in time convolution must be even integers\")\n",
    "\n",
    "        self.num_of_filters = num_of_filters\n",
    "        with self.name_scope():\n",
    "            self.conv = nn.Conv2D(channels=num_of_filters,\n",
    "                                  kernel_size=(1, K_t))\n",
    "            self.residual_conv = nn.Conv2D(channels=num_of_filters // 2,\n",
    "                                           kernel_size=(1, K_t))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: mx.ndarray, shape is\n",
    "            (batch_size, num_of_features, num_of_vertices, num_of_timesteps)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mx.ndarray, shape is\n",
    "            (batch_size, num_of_filters/2,\n",
    "             num_of_vertices, num_of_timesteps - K_t)\n",
    "\n",
    "        '''\n",
    "\n",
    "        # shape is\n",
    "        # (batch_size, num_of_filters, num_of_vertices, num_of_timesteps - K_t)\n",
    "        conv_output = self.conv(x)\n",
    "\n",
    "        P = conv_output[:, : self.num_of_filters // 2, :, :]\n",
    "        Q = conv_output[:, self.num_of_filters // 2:, :, :]\n",
    "        assert P.shape == Q.shape\n",
    "\n",
    "        return (P + self.residual_conv(x)) * nd.sigmoid(Q)\n",
    "\n",
    "\n",
    "class ST_block(nn.Block):\n",
    "    def __init__(self, backbone, **kwargs):\n",
    "        super(ST_block, self).__init__(**kwargs)\n",
    "\n",
    "        # number of first temporal convolution's filters\n",
    "        num_of_time_conv_filters1 = backbone['num_of_time_conv_filters1']\n",
    "\n",
    "        # number of second temporal convolution's filters\n",
    "        num_of_time_conv_filters2 = backbone['num_of_time_conv_filters2']\n",
    "\n",
    "        # length of temporal convolutional filter\n",
    "        K_t = backbone['K_t']\n",
    "\n",
    "        # number of spatial convolution's filters\n",
    "        num_of_cheb_filters = backbone['num_of_cheb_filters']\n",
    "\n",
    "        # number of the order of chebNet\n",
    "        K = backbone['K']\n",
    "\n",
    "        # list of chebyshev polynomials from first-order to K-order\n",
    "        cheb_polys = backbone['cheb_polys']\n",
    "\n",
    "        with self.name_scope():\n",
    "            self.time_conv1 = temporal_conv_layer(\n",
    "                num_of_time_conv_filters1, K_t)\n",
    "            self.cheb_conv = cheb_conv(num_of_cheb_filters, K, cheb_polys)\n",
    "            self.time_conv2 = temporal_conv_layer(\n",
    "                num_of_time_conv_filters2, K_t)\n",
    "            self.ln = nn.LayerNorm(axis=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: mx.ndarray, shape is\n",
    "            (batch_size, num_of_features, num_of_vertices, num_of_timesteps)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mx.ndarray, shape is\n",
    "            (batch_size, num_of_time_conv_filters2 / 2, num_of_vertices,\n",
    "             num_of_timesteps - 2(K_t - 1) )\n",
    "\n",
    "        '''\n",
    "        return self.ln(self.time_conv2(self.cheb_conv(self.time_conv1(x))))\n",
    "\n",
    "\n",
    "class STGCN(nn.Block):\n",
    "    def __init__(self, backbones, num_of_last_time_conv_filters, **kwargs):\n",
    "        super(STGCN, self).__init__(**kwargs)\n",
    "\n",
    "        # two ST blocks\n",
    "        self.st_blocks = []\n",
    "        for backbone in backbones:\n",
    "            self.st_blocks.append(ST_block(backbone))\n",
    "            self.register_child(self.st_blocks[-1])\n",
    "\n",
    "        # extra three convolutional structure to map output into label space\n",
    "        with self.name_scope():\n",
    "            self.last_time_conv = temporal_conv_layer(\n",
    "                num_of_last_time_conv_filters, 4)\n",
    "            self.final_conv = nn.Conv2D(channels=128, kernel_size=(1, 1),\n",
    "                                        activation='sigmoid')\n",
    "            self.conv_output = nn.Conv2D(channels=1, kernel_size=(1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        x: mx.ndarray, shape is\n",
    "            (batch_size, num_of_features, num_of_vertices, num_of_timesteps)\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        mx.ndarray, shape is\n",
    "            (batch_size, num_of_vertices, num_points_for_prediction)\n",
    "        '''\n",
    "        output = x\n",
    "        for block in self.st_blocks:\n",
    "            output = block(output)\n",
    "        return self.conv_output(\n",
    "            self.final_conv(self.last_time_conv(output)))[:, 0, :, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx = mx.cpu()\n",
    "num_of_vertices = 307\n",
    "A, indices_dict = get_adj_matrix(\n",
    "    'data/test_data1/distance.csv', num_of_vertices)\n",
    "graph_signal_filename = 'data/test_data1/graph_signal_data_small.txt'\n",
    "X = data_preprocess(indices_dict, graph_signal_filename)\n",
    "L_tilde = scaled_Laplacian(A)\n",
    "cheb_polys = [nd.array(i, ctx=ctx) for i in cheb_polynomial(L_tilde, 3)]\n",
    "backbones = [\n",
    "    {\n",
    "        'num_of_time_conv_filters1': 32,\n",
    "        'num_of_time_conv_filters2': 64,\n",
    "        'K_t': 3,\n",
    "        'num_of_cheb_filters': 32,\n",
    "        'K': 1,\n",
    "        'cheb_polys': cheb_polys\n",
    "    },\n",
    "    {\n",
    "        'num_of_time_conv_filters1': 32,\n",
    "        'num_of_time_conv_filters2': 128,\n",
    "        'K_t': 3,\n",
    "        'num_of_cheb_filters': 32,\n",
    "        'K': 1,\n",
    "        'cheb_polys': cheb_polys\n",
    "    }\n",
    "]\n",
    "net = STGCN(backbones, 128)\n",
    "net.initialize(ctx=ctx)\n",
    "output = net(nd.random_uniform(shape=(16, 1, num_of_vertices, 12), ctx=ctx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import os\n",
    "import configparser\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "from mxnet.gluon import Trainer\n",
    "\n",
    "from lib import utils\n",
    "from model import STGCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read configuration file: configurations/config.conf\n"
     ]
    }
   ],
   "source": [
    "config = configparser.ConfigParser()\n",
    "config_file = 'configurations/config.conf\n",
    "print('read configuration file: {}'.format(config_file))\n",
    "config.read(config_file)\n",
    "\n",
    "data_configs = config['Data']\n",
    "\n",
    "adj_filename = data_configs['adj_filename']\n",
    "graph_signal_filename = data_configs['graph_signal_filename']\n",
    "num_of_vertices = int(data_configs['num_of_vertices'])\n",
    "\n",
    "model_configs = config['Model']\n",
    "\n",
    "orders_of_cheb = int(model_configs['orders_of_cheb'])\n",
    "\n",
    "ctx = model_configs['ctx']\n",
    "if ctx.startswith('cpu'):\n",
    "    ctx = mx.cpu()\n",
    "elif ctx.startswith('gpu'):\n",
    "    ctx = mx.gpu(int(ctx[ctx.find('-') + 1:]))\n",
    "else:\n",
    "    raise ValueError(\"context seems to be wrong!\")\n",
    "\n",
    "optimizer = model_configs['optimizer']\n",
    "learning_rate = float(model_configs['learning_rate'])\n",
    "\n",
    "batch_size = model_configs['batch_size']\n",
    "epochs = int(model_configs['epochs'])\n",
    "\n",
    "decay_interval = model_configs['decay_interval']\n",
    "if decay_interval == 'None':\n",
    "    decay_interval = None\n",
    "decay_rate = model_configs['decay_rate']\n",
    "if decay_rate == 'None':\n",
    "    decay_rate = None\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data shape: (288, 3, 307, 12) (288, 307, 1)\n",
      "validation data shape: (88, 3, 307, 12) (88, 307, 1)\n",
      "testing data shape: (88, 3, 307, 12) (88, 307, 1)\n",
      "None None\n",
      "288 <class 'int'>\n",
      "20265.2\n",
      "current epoch is 1\n",
      "training loss(MSE): 20265.19921875\n",
      "validation loss(MSE): 26410.96875\n",
      "testing loss(MSE): 57564.43359375\n",
      "time: 12.715973377227783\n",
      "\n",
      "288 <class 'int'>\n",
      "20243.057\n",
      "current epoch is 2\n",
      "training loss(MSE): 20243.056640625\n",
      "validation loss(MSE): 26383.349609375\n",
      "testing loss(MSE): 57515.78515625\n",
      "time: 14.370221138000488\n",
      "\n",
      "288 <class 'int'>\n",
      "20216.557\n",
      "current epoch is 3\n",
      "training loss(MSE): 20216.556640625\n",
      "validation loss(MSE): 26354.701171875\n",
      "testing loss(MSE): 57465.62890625\n",
      "time: 13.80705213546753\n",
      "\n",
      "288 <class 'int'>\n",
      "20189.637\n",
      "current epoch is 4\n",
      "training loss(MSE): 20189.63671875\n",
      "validation loss(MSE): 26320.279296875\n",
      "testing loss(MSE): 57403.93359375\n",
      "time: 12.597464561462402\n",
      "\n",
      "288 <class 'int'>\n",
      "20157.555\n",
      "current epoch is 5\n",
      "training loss(MSE): 20157.5546875\n",
      "validation loss(MSE): 26279.9296875\n",
      "testing loss(MSE): 57331.31640625\n",
      "time: 12.250782251358032\n",
      "\n",
      "288 <class 'int'>\n",
      "20120.008\n",
      "current epoch is 6\n",
      "training loss(MSE): 20120.0078125\n",
      "validation loss(MSE): 26235.28125\n",
      "testing loss(MSE): 57250.84765625\n",
      "time: 11.395501852035522\n",
      "\n",
      "288 <class 'int'>\n",
      "20078.445\n",
      "current epoch is 7\n",
      "training loss(MSE): 20078.4453125\n",
      "validation loss(MSE): 26187.408203125\n",
      "testing loss(MSE): 57164.546875\n",
      "time: 11.990803003311157\n",
      "\n",
      "288 <class 'int'>\n",
      "20033.828\n",
      "current epoch is 8\n",
      "training loss(MSE): 20033.828125\n",
      "validation loss(MSE): 26137.794921875\n",
      "testing loss(MSE): 57075.0625\n",
      "time: 11.21216630935669\n",
      "\n",
      "288 <class 'int'>\n",
      "19987.564\n",
      "current epoch is 9\n",
      "training loss(MSE): 19987.564453125\n",
      "validation loss(MSE): 26088.744140625\n",
      "testing loss(MSE): 56986.5625\n",
      "time: 8.478996515274048\n",
      "\n",
      "288 <class 'int'>\n",
      "19941.824\n",
      "current epoch is 10\n",
      "training loss(MSE): 19941.82421875\n",
      "validation loss(MSE): 26042.501953125\n",
      "testing loss(MSE): 56903.109375\n",
      "time: 8.321899890899658\n",
      "\n",
      "288 <class 'int'>\n",
      "19898.713\n",
      "current epoch is 11\n",
      "training loss(MSE): 19898.712890625\n",
      "validation loss(MSE): 26000.83203125\n",
      "testing loss(MSE): 56827.875\n",
      "time: 8.570228338241577\n",
      "\n",
      "288 <class 'int'>\n",
      "19859.867\n",
      "current epoch is 12\n",
      "training loss(MSE): 19859.8671875\n",
      "validation loss(MSE): 25964.701171875\n",
      "testing loss(MSE): 56762.62109375\n",
      "time: 10.11925196647644\n",
      "\n",
      "288 <class 'int'>\n",
      "19826.188\n",
      "current epoch is 13\n",
      "training loss(MSE): 19826.1875\n",
      "validation loss(MSE): 25934.14453125\n",
      "testing loss(MSE): 56707.40234375\n",
      "time: 13.332823038101196\n",
      "\n",
      "288 <class 'int'>\n",
      "19797.707\n",
      "current epoch is 14\n",
      "training loss(MSE): 19797.70703125\n",
      "validation loss(MSE): 25908.46875\n",
      "testing loss(MSE): 56660.97265625\n",
      "time: 11.413729667663574\n",
      "\n",
      "288 <class 'int'>\n",
      "19773.773\n",
      "current epoch is 15\n",
      "training loss(MSE): 19773.7734375\n",
      "validation loss(MSE): 25886.66796875\n",
      "testing loss(MSE): 56621.52734375\n",
      "time: 11.531955242156982\n",
      "\n",
      "288 <class 'int'>\n",
      "19753.453\n",
      "current epoch is 16\n",
      "training loss(MSE): 19753.453125\n",
      "validation loss(MSE): 25867.767578125\n",
      "testing loss(MSE): 56587.32421875\n",
      "time: 10.54419994354248\n",
      "\n",
      "288 <class 'int'>\n",
      "19735.84\n",
      "current epoch is 17\n",
      "training loss(MSE): 19735.83984375\n",
      "validation loss(MSE): 25850.966796875\n",
      "testing loss(MSE): 56556.91015625\n",
      "time: 11.018216609954834\n",
      "\n",
      "288 <class 'int'>\n",
      "19720.18\n",
      "current epoch is 18\n",
      "training loss(MSE): 19720.1796875\n",
      "validation loss(MSE): 25835.607421875\n",
      "testing loss(MSE): 56529.09765625\n",
      "time: 12.079156398773193\n",
      "\n",
      "288 <class 'int'>\n",
      "19705.87\n",
      "current epoch is 19\n",
      "training loss(MSE): 19705.869140625\n",
      "validation loss(MSE): 25821.193359375\n",
      "testing loss(MSE): 56502.98828125\n",
      "time: 12.137702941894531\n",
      "\n",
      "288 <class 'int'>\n",
      "19692.44\n",
      "current epoch is 20\n",
      "training loss(MSE): 19692.439453125\n",
      "validation loss(MSE): 25807.3515625\n",
      "testing loss(MSE): 56477.9140625\n",
      "time: 9.865913391113281\n",
      "\n",
      "288 <class 'int'>\n",
      "19679.543\n",
      "current epoch is 21\n",
      "training loss(MSE): 19679.54296875\n",
      "validation loss(MSE): 25793.814453125\n",
      "testing loss(MSE): 56453.38671875\n",
      "time: 9.483912706375122\n",
      "\n",
      "288 <class 'int'>\n",
      "19666.93\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6b52c852a7cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesting_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_interval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     verbose=True)\n\u001b[0m",
      "\u001b[0;32m/mxnet/lib/utils.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(net, training_dataloader, validation_dataloader, testing_dataloader, epochs, loss_function, trainer, decay_interval, decay_rate, verbose)\u001b[0m\n\u001b[1;32m    237\u001b[0m             test_loss_list_tmp.append(\n\u001b[1;32m    238\u001b[0m                 loss_function(net(x), y).mean().asscalar())\n\u001b[0;32m--> 239\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         test_loss_list.append(\n\u001b[1;32m    241\u001b[0m             sum(test_loss_list_tmp) / len(test_loss_list_tmp))\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masscalar\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The current array is not a scalar\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/mxnet/ndarray/ndarray.py\u001b[0m in \u001b[0;36masnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1978\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1979\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_void_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1980\u001b[0;31m             ctypes.c_size_t(data.size)))\n\u001b[0m\u001b[1;32m   1981\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "A, indices_dict = utils.get_adj_matrix(adj_filename, num_of_vertices)\n",
    "X = utils.data_preprocess(indices_dict, graph_signal_filename)\n",
    "\n",
    "L_tilde = utils.scaled_Laplacian(A)\n",
    "cheb_polys = [nd.array(i, ctx=ctx)\n",
    "              for i in utils.cheb_polynomial(L_tilde, orders_of_cheb)]\n",
    "\n",
    "# training: validation: testing = 6: 2: 2\n",
    "split_line1 = int(X.shape[2] * 0.6)\n",
    "split_line2 = int(X.shape[2] * 0.8)\n",
    "\n",
    "train_original_data = X[:, :, : split_line1]\n",
    "val_original_data = X[:, :, split_line1: split_line2]\n",
    "test_original_data = X[:, :, split_line2:]\n",
    "\n",
    "training_data, training_target = utils.build_dataset(\n",
    "    train_original_data, 12, 1)\n",
    "\n",
    "val_data, val_target = utils.build_dataset(\n",
    "    val_original_data, 12, 1)\n",
    "\n",
    "testing_data, testing_target = utils.build_dataset(\n",
    "    test_original_data, 12, 1)\n",
    "\n",
    "# Z-score preprocessing\n",
    "assert num_of_vertices == training_data.shape[2]\n",
    "_, num_of_features, _, _ = training_data.shape\n",
    "\n",
    "transformer = StandardScaler()\n",
    "training_data_norm = transformer.fit_transform(\n",
    "    training_data.reshape(training_data.shape[0], -1))\\\n",
    "    .reshape(training_data.shape[0], num_of_features, num_of_vertices, 12)\n",
    "val_data_norm = transformer.transform(\n",
    "    val_data.reshape(val_data.shape[0], -1))\\\n",
    "    .reshape(val_data.shape[0], num_of_features, num_of_vertices, 12)\n",
    "testing_data_norm = transformer.transform(\n",
    "    testing_data.reshape(testing_data.shape[0], -1))\\\n",
    "    .reshape(testing_data.shape[0], num_of_features, num_of_vertices, 12)\n",
    "\n",
    "training_data_norm = nd.array(training_data_norm, ctx=ctx)\n",
    "val_data_norm = nd.array(val_data_norm, ctx=ctx)\n",
    "testing_data_norm = nd.array(testing_data_norm, ctx=ctx)\n",
    "\n",
    "training_target = nd.array(training_target, ctx=ctx)\n",
    "val_target = nd.array(val_target, ctx=ctx)\n",
    "testing_target = nd.array(testing_target, ctx=ctx)\n",
    "\n",
    "print('training data shape:',\n",
    "      training_data_norm.shape, training_target.shape)\n",
    "print('validation data shape:', val_data_norm.shape, val_target.shape)\n",
    "print('testing data shape:', testing_data_norm.shape, testing_target.shape)\n",
    "\n",
    "# model initialization\n",
    "backbones = [\n",
    "    {\n",
    "        'num_of_time_conv_filters1': 32,\n",
    "        'num_of_time_conv_filters2': 64,\n",
    "        'K_t': 3,\n",
    "        'num_of_cheb_filters': 32,\n",
    "        'K': 1,\n",
    "        'cheb_polys': cheb_polys\n",
    "    },\n",
    "    {\n",
    "        'num_of_time_conv_filters1': 32,\n",
    "        'num_of_time_conv_filters2': 128,\n",
    "        'K_t': 3,\n",
    "        'num_of_cheb_filters': 32,\n",
    "        'K': 1,\n",
    "        'cheb_polys': cheb_polys\n",
    "    }\n",
    "]\n",
    "net = STGCN(backbones, 128)\n",
    "net.initialize(ctx=ctx)\n",
    "\n",
    "loss_function = gluon.loss.L2Loss()\n",
    "\n",
    "trainer = Trainer(net.collect_params(), optimizer)\n",
    "trainer.set_learning_rate(learning_rate)\n",
    "training_dataloader = gluon.data.DataLoader(\n",
    "    gluon.data.ArrayDataset(training_data_norm, training_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "validation_dataloader = gluon.data.DataLoader(\n",
    "    gluon.data.ArrayDataset(val_data_norm, val_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "testing_dataloader = gluon.data.DataLoader(\n",
    "    gluon.data.ArrayDataset(testing_data_norm, testing_target),\n",
    "    batch_size=batch_size, shuffle=False)\n",
    "\n",
    "if not os.path.exists('stgcn_params'):\n",
    "    os.mkdir('stgcn_params')\n",
    "\n",
    "print(decay_interval, decay_rate)\n",
    "train_loss_list, val_loss_list, test_loss_list = utils.train_model(\n",
    "    net, training_dataloader, validation_dataloader, testing_dataloader,\n",
    "    epochs, loss_function, trainer, decay_interval, decay_rate,\n",
    "    verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
